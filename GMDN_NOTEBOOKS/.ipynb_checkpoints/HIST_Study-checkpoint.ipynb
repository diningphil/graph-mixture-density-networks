{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For RAND baseline, it suffices to compute log(1/n) where n is the number of nodes in the graph\n",
    "\n",
    "### if the regression target is continuous and multivariate, we create an uniform distribution for all output dimensions, then we multiply all dimensions' probabilities together and take the natural log of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from pydoc import locate\n",
    "from pydgn.experiment.experiment import s2c\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pydgn.experiment.experiment import Experiment\n",
    "\n",
    "from pydgn.evaluation.grid import Grid\n",
    "from pydgn.data.provider import DataProvider\n",
    "from gmdn_dataset import BarabasiAlbertDataset, ErdosRenyiDataset, TUDatasetInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_folder = Path('SPLITS')\n",
    "\n",
    "num_workers = 0\n",
    "\n",
    "barabasi_albert_root = Path('DATA', 'BARABASI_ALBERT')\n",
    "erdos_renyi_root = Path('DATA', 'ERDOS_RENYI')\n",
    "tudataset_root = Path('DATA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Barabasi-Albert dataset loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_barabasi_albert(size, connectivity):\n",
    "    dataset_name = Path(f'barabasi_albert_{size}_{connectivity}')\n",
    "    if not os.path.exists(barabasi_albert_root / dataset_name):\n",
    "        print(\"Create your dataset first\")\n",
    "        return None\n",
    "    else:\n",
    "        d = BarabasiAlbertDataset(barabasi_albert_root, 'barabasi_albert', size, connectivity)\n",
    "        batch_size = len(d)\n",
    "        shuffle = False\n",
    "        dataset_getter_class = s2c('pydgn.data.provider.DataProvider')\n",
    "        dataset_getter = dataset_getter_class(barabasi_albert_root,\n",
    "                                              'SPLITS',\n",
    "                                              s2c('data.dataset.BarabasiAlbertDataset'),\n",
    "                                              dataset_name,\n",
    "                                              1, # outer_folds\n",
    "                                              1, # inner folds\n",
    "                                              0, # num_workers\n",
    "                                              False)  # pin memory\n",
    "        dataset_getter.set_outer_k(0)\n",
    "        dataset_getter.set_inner_k(0)\n",
    "\n",
    "        # Instantiate the Dataset Loaders\n",
    "        train_loader = dataset_getter.get_outer_train(batch_size=batch_size, shuffle=shuffle)\n",
    "        val_loader = dataset_getter.get_outer_val(batch_size=batch_size, shuffle=shuffle)\n",
    "        test_loader = dataset_getter.get_outer_test(batch_size=batch_size, shuffle=shuffle)\n",
    "                \n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Istantiate Erdos-Renyi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_erdos_renyi(size, connectivity):\n",
    "    dataset_name = Path(f'erdos_renyi_{size}_{connectivity}')\n",
    "    if not os.path.exists(erdos_renyi_root / dataset_name):\n",
    "        print(\"Create your dataset first\")\n",
    "        return None\n",
    "    else:\n",
    "        d = ErdosRenyiDataset(erdos_renyi_root, 'erdos_renyi', size, connectivity)\n",
    "        batch_size = len(d)\n",
    "        shuffle = False\n",
    "        dataset_getter_class = s2c('pydgn.data.provider.DataProvider')\n",
    "        dataset_getter = dataset_getter_class(erdos_renyi_root,\n",
    "                                              'SPLITS',\n",
    "                                              s2c('data.dataset.ErdosRenyiDataset'),\n",
    "                                              dataset_name,\n",
    "                                              1, # outer_folds\n",
    "                                              1, # inner folds\n",
    "                                              0, # num_workers\n",
    "                                              False)  # pin memory\n",
    "        dataset_getter.set_outer_k(0)\n",
    "        dataset_getter.set_inner_k(0)\n",
    "\n",
    "        # Instantiate the Dataset Loaders\n",
    "        train_loader = dataset_getter.get_outer_train(batch_size=batch_size, shuffle=shuffle)\n",
    "        val_loader = dataset_getter.get_outer_val(batch_size=batch_size, shuffle=shuffle)\n",
    "        test_loader = dataset_getter.get_outer_test(batch_size=batch_size, shuffle=shuffle)\n",
    "                \n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Istantiate TuDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TUDataset(dataset_name):\n",
    "\n",
    "    if not os.path.exists(tudataset_root / dataset_name):\n",
    "        print(\"Create your dataset first\")\n",
    "        return None\n",
    "    else:\n",
    "        d = TUDatasetInterface(tudataset_root, dataset_name, use_node_attr=True)\n",
    "        batch_size = len(d)\n",
    "        shuffle = False\n",
    "        dataset_getter_class = s2c('pydgn.data.provider.DataProvider')\n",
    "        dataset_getter = dataset_getter_class(tudataset_root,\n",
    "                                              'SPLITS',\n",
    "                                              s2c('data.dataset.TUDatasetInterface'),\n",
    "                                              dataset_name,\n",
    "                                              1, # outer_folds\n",
    "                                              1, # inner folds\n",
    "                                              0, # num_workers\n",
    "                                              False)  # pin memory\n",
    "        dataset_getter.set_outer_k(0)\n",
    "        dataset_getter.set_inner_k(0)\n",
    "\n",
    "        # Instantiate the Dataset Loaders\n",
    "        train_loader = dataset_getter.get_outer_train(batch_size=batch_size, shuffle=shuffle)\n",
    "        val_loader = dataset_getter.get_outer_val(batch_size=batch_size, shuffle=shuffle)\n",
    "        test_loader = dataset_getter.get_outer_test(batch_size=batch_size, shuffle=shuffle)\n",
    "                \n",
    "        return d, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a dataset and get the mean log likelihood using a model of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barabasi-Albert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIST performance on TRAIN is  tensor(-1.1418)\n",
      "HIST performance on VAL is  tensor(-1.1545)\n",
      "HIST performance on TEST is  tensor(-1.1607)\n"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "connectivity= \"2@5@10@20\"\n",
    "\n",
    "barabasi_albert_name = Path(f'barabasi_albert_{size}_{connectivity}')\n",
    "# Get full batch\n",
    "barabasi_albert_train_loader, barabasi_albert_val_loader, barabasi_albert_test_loader = get_barabasi_albert(size, connectivity)\n",
    "\n",
    "# NB: size of graphs is fixed\n",
    "for train_data in barabasi_albert_train_loader:\n",
    "    train_y = train_data.y\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for val_data in barabasi_albert_val_loader:\n",
    "    val_y = val_data.y\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for test_data in barabasi_albert_test_loader:\n",
    "    test_y = test_data.y\n",
    "    \n",
    "train_bins = torch.bincount(train_y.squeeze())\n",
    "normalized_train_bins = train_bins.float()/torch.sum(train_bins)\n",
    "# sns.distplot(train_y.squeeze().numpy(), bins=100, kde=True)\n",
    "\n",
    "p_y = normalized_train_bins[train_y] + 1e-8\n",
    "print(\"HIST performance on TRAIN is \", p_y.log().mean())\n",
    "\n",
    "p_y = normalized_train_bins[val_y] + 1e-8\n",
    "print(\"HIST performance on VAL is \", p_y.log().mean())\n",
    "\n",
    "p_y = normalized_train_bins[test_y] + 1e-8\n",
    "print(\"HIST performance on TEST is \", p_y.log().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erdos-Renyi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIST performance on TRAIN is  tensor(-2.3129)\n",
      "HIST performance on VAL is  tensor(-2.2881)\n",
      "HIST performance on TEST is  tensor(-2.3249)\n"
     ]
    }
   ],
   "source": [
    "size = 100\n",
    "connectivity= \"0.01@0.05@0.1@0.2\"\n",
    "erdos_renyi_name = Path(f'erdos_renyi_{size}_{connectivity}')\n",
    "erdos_renyi_train_loader, erdos_renyi_val_loader, erdos_renyi_test_loader = get_erdos_renyi(size, connectivity)\n",
    "\n",
    "# NB: size of graphs is fixed\n",
    "for train_data in erdos_renyi_train_loader:\n",
    "    train_y = train_data.y\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for val_data in erdos_renyi_val_loader:\n",
    "    val_y = val_data.y\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for test_data in erdos_renyi_test_loader:\n",
    "    test_y = test_data.y\n",
    "    \n",
    "train_bins = torch.bincount(train_y.squeeze())\n",
    "normalized_train_bins = train_bins.float()/torch.sum(train_bins)\n",
    "# sns.distplot(train_y.squeeze().numpy(), bins=100, kde=True)\n",
    "\n",
    "p_y = normalized_train_bins[train_y] + 1e-8\n",
    "print(\"HIST performance on TRAIN is \", p_y.log().mean())\n",
    "\n",
    "p_y = normalized_train_bins[val_y] + 1e-8\n",
    "print(\"HIST performance on VAL is \", p_y.log().mean())\n",
    "\n",
    "p_y = normalized_train_bins[test_y] + 1e-8\n",
    "print(\"HIST performance on TEST is \", p_y.log().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alchemy_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'alchemy_full'\n",
    "dataset, train_loader, val_loader, test_loader = get_TUDataset(dataset_name)\n",
    "\n",
    "max_val, _ = dataset.data.y.max(dim=0)\n",
    "min_val, _ = dataset.data.y.min(dim=0)\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for train_data in train_loader:\n",
    "    train_y = train_data.y\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for val_data in val_loader:\n",
    "    val_y = val_data.y\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for test_data in test_loader:\n",
    "    test_y = test_data.y\n",
    "\n",
    "\n",
    "# BEST CHOSEN BY TRIALS WHILE LOOKING AT THE VALIDATION SCORE, THEN TEST SCORE WAS CHECKED\n",
    "no_bins = 17\n",
    "\n",
    "train_p_y_by_component = []\n",
    "val_p_y_by_component = []\n",
    "test_p_y_by_component = []\n",
    "for i in range(train_y.shape[1]):    \n",
    "    # print(f'Component {i+1}')\n",
    "    \n",
    "    bin_size = (max_val[i]-min_val[i])/no_bins\n",
    "    train_hist = torch.histc(train_y[:,i], bins=no_bins, min=min_val[i], max=max_val[i])\n",
    "    normalized_train_hist = train_hist.float()/torch.sum(train_bins)\n",
    "\n",
    "    train_bin_assignment = ((train_y[:,i]-min_val[i])//bin_size).long() - 1\n",
    "    val_bin_assignment = ((val_y[:,i]-min_val[i])//bin_size).long() - 1\n",
    "    test_bin_assignment = ((test_y[:,i]-min_val[i])//bin_size).long() - 1\n",
    "\n",
    "    p_y = normalized_train_hist[train_bin_assignment] + 1e-8\n",
    "    train_p_y_by_component.append(p_y.log())\n",
    "    # print(f\"HIST performance on TRAIN for component {i+1} is \", p_y.log().mean())\n",
    "  \n",
    "    p_y = normalized_train_hist[val_bin_assignment] + 1e-8\n",
    "    val_p_y_by_component.append(p_y.log())\n",
    "    # print(f\"HIST performance on VAL for component {i+1} is \", p_y.log().mean())\n",
    "\n",
    "    p_y = normalized_train_hist[test_bin_assignment] + 1e-8\n",
    "    test_p_y_by_component.append(p_y.log())\n",
    "    # print(f\"HIST performance on TEST for component {i+1} is \", p_y.log().mean())\n",
    "\n",
    "train_p_y  = torch.stack(train_p_y_by_component, dim=1)\n",
    "val_p_y  = torch.stack(val_p_y_by_component, dim=1)\n",
    "test_p_y  = torch.stack(test_p_y_by_component, dim=1)\n",
    "    \n",
    "print(\"HIST performance on TRAIN is \", train_p_y.sum(dim=1).mean())\n",
    "print(\"HIST performance on VAL is \", val_p_y.sum(dim=1).mean())\n",
    "print(\"HIST performance on TEST is \", test_p_y.sum(dim=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ZINC_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1\n",
      "HIST performance on TRAIN is  tensor(-1.2752)\n",
      "HIST performance on VAL is  tensor(-1.2793)\n",
      "HIST performance on TEST is  tensor(-1.2841)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'ZINC_full'\n",
    "dataset, train_loader, val_loader, test_loader = get_TUDataset(dataset_name)\n",
    "\n",
    "max_val, _ = dataset.data.y.max(dim=0)\n",
    "min_val, _ = dataset.data.y.min(dim=0)\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for train_data in train_loader:\n",
    "    train_y = train_data.y\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for val_data in val_loader:\n",
    "    val_y = val_data.y\n",
    "    \n",
    "# NB: size of graphs is fixed\n",
    "for test_data in test_loader:\n",
    "    test_y = test_data.y\n",
    "\n",
    "\n",
    "# BEST CHOSEN BY TRIALS WHILE LOOKING AT THE VALIDATION SCORE, THEN TEST SCORE WAS CHECKED\n",
    "no_bins = 31\n",
    "\n",
    "train_p_y_by_component = []\n",
    "val_p_y_by_component = []\n",
    "test_p_y_by_component = []\n",
    "for i in range(train_y.shape[1]):    \n",
    "    print(f'Component {i+1}')\n",
    "    \n",
    "    bin_size = (max_val[i]-min_val[i])/no_bins\n",
    "    train_hist = torch.histc(train_y[:,i], bins=no_bins, min=min_val[i], max=max_val[i])\n",
    "    normalized_train_hist = train_hist.float()/torch.sum(train_bins)\n",
    "\n",
    "    train_bin_assignment = ((train_y[:,i]-min_val[i])//bin_size).long() - 1\n",
    "    val_bin_assignment = ((val_y[:,i]-min_val[i])//bin_size).long() - 1\n",
    "    test_bin_assignment = ((test_y[:,i]-min_val[i])//bin_size).long() - 1\n",
    "\n",
    "    p_y = normalized_train_hist[train_bin_assignment] + 1e-8\n",
    "    train_p_y_by_component.append(p_y.log())\n",
    "  \n",
    "    p_y = normalized_train_hist[val_bin_assignment] + 1e-8\n",
    "    val_p_y_by_component.append(p_y.log())\n",
    "\n",
    "    p_y = normalized_train_hist[test_bin_assignment] + 1e-8\n",
    "    test_p_y_by_component.append(p_y.log())\n",
    "\n",
    "\n",
    "train_p_y  = torch.stack(train_p_y_by_component, dim=1)\n",
    "val_p_y  = torch.stack(val_p_y_by_component, dim=1)\n",
    "test_p_y  = torch.stack(test_p_y_by_component, dim=1)\n",
    "    \n",
    "print(\"HIST performance on TRAIN is \", train_p_y.sum(dim=1).mean())\n",
    "print(\"HIST performance on VAL is \", val_p_y.sum(dim=1).mean())\n",
    "print(\"HIST performance on TEST is \", test_p_y.sum(dim=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
